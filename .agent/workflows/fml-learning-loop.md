---
description: FML Learning & Continuous Optimization Framework
---

# FML Learning Loop Workflow

## Objective
Implement a Reinforcement Learning (RL) style feedback loop to ensure every bug fix and optimization improves the framework and future applications.

## 1. The Debugging Log (Memory)
Whenever a non-trivial bug is fixed (e.g., tool mismatch, visualization error, coordinate logic), the developer/AI MUST:
1. Open `docs/learning/debug_log.md` in the current app.
2. Add a entry with:
   - **Bug Description**: What was failing?
   - **Root Cause**: Why did it fail? (e.g., "Mafs import missing", "Token limit hit")
   - **Fix Applied**: How was it resolved?
   - **Prevention Rule**: What rule should be added to the framework to prevent this?

## 2. Framework Feedback (RL Optimization)
Once a month, or after a major feature release:
1. Review all `docs/learning/debug_log.md` entries.
2. **Update Templates**: If the bug was caused by a missing import or bad default, update the `fml-framework/templates/` files.
3. **Update Workflows**: If the bug was a naming or logic error, update the `.agent/workflows/` files with new strict rules.
4. **Optimize Prompts**: If the AI missed a tool or was inefficient, update the `systemPrompt.js` baseline in the framework.

## 3. RL for Performance
- **Token Tracking**: Monitor the average token count of `systemPrompt.js`. 
- **Goal**: Keep it under 1000 tokens while maintaining accuracy.
- **Action**: If tokens exceed 1500, trigger a mandatory "Prompt Optimization" pass using the `fml4-o` strategy.

## 4. Verification of Learning
- Check that new apps generated by the framework do NOT contain the bugs documented in the latest logs.
- Successful prevention of a known bug = **Positive Reinforcement** (Framework is working).
- Recurrence of a documented bug = **Negative Reinforcement** (Workflow needs stricter wording).
